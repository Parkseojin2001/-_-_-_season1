{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcehqVtvabKgZ5xMtif8/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parkseojin2001/DeepLearningZeroToAll/blob/main/lab_02_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ],
      "metadata": {
        "id": "3ifc6Gtp5utL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis\n",
        "\n",
        "$$ H(x) = Wx + b $$\n",
        "\n",
        "$W$ : 가중치(weight)\n",
        "\n",
        "$b$ : 편향(bias)"
      ],
      "metadata": {
        "id": "tUFAKczL56Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "-9cZPo9d6X8z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data definition\n",
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2], [4], [6]])"
      ],
      "metadata": {
        "id": "38oR9R4Y54W5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weight와 Bias를 0으로 초기화\n",
        "W = torch.zeros(1, requires_grad = True)  # requires_grad = True 학습 명시\n",
        "b = torch.zeros(1, requires_grad = True)\n",
        "hypothesis = x_train * W + b"
      ],
      "metadata": {
        "id": "F_XLeWXQ6xTo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute loss\n",
        "\n",
        "**Mean squared Error(MSE)**\n",
        "\n",
        "$$cost(W,b) = \\frac{1}{m} \\sum_{i = 1}^m((H(x^{(i)}) - y^{(i)})^2$$\n",
        "\n",
        "- $H(x^{(i)})$ : Prediction\n",
        "\n",
        "- $y^{(i)}$ : Target"
      ],
      "metadata": {
        "id": "GrOd03nL7lKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cost = torch.mean((hypothesis - y_train)**2)"
      ],
      "metadata": {
        "id": "rHt5iTip7jpu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient descent"
      ],
      "metadata": {
        "id": "LaMiu-2R8nq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.optim 라이브러리 사용\n",
        "# [W, b] : 학습할 tensor\n",
        "# lr = 0.01 : learning rate\n",
        "optimizer = optim.SGD([W, b], lr = 0.01)\n",
        "\n",
        "optimizer.zero_grad() # zero_grad()로 gradient 초기화\n",
        "cost.backward() # backward()로 gradient 계산\n",
        "optimizer.step()  # step()으로 개선"
      ],
      "metadata": {
        "id": "gkAqP5Mu8mOM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full training code"
      ],
      "metadata": {
        "id": "wq7OTL4O-GF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.FloatTensor([[1], [2], [3]])\n",
        "y_train = torch.FloatTensor([[2],[4], [6]])\n",
        "\n",
        "W = torch.zeros(1, requires_grad = True)\n",
        "b = torch.zeros(1, requires_grad = True)\n",
        "\n",
        "optimizer = optim.SGD([W, b], lr = 0.01)\n",
        "\n",
        "nb_epochs = 1000\n",
        "for epoch in range(1, nb_epochs + 1):\n",
        "  hypothesis = x_train * W + b\n",
        "  cost = torch.mean((hypothesis - y_train) ** 2)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  cost.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "id": "CwJORpBi9b5f"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}