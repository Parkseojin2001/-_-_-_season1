{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ4kMQMzalisI0tY9ni/W9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parkseojin2001/DeepLearningZeroToAll/blob/main/lab_04_2_load_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "_k9O1MmERUAp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data\n",
        "\n",
        "- 엄청난 양의 데이터를 한 번에 학습시킬 수 없음\n",
        "\n",
        "  - 너무 느리다.\n",
        "  \n",
        "  - 하드웨어적으로 불가능\n",
        "\n",
        "**해결책 : Minibatch**"
      ],
      "metadata": {
        "id": "gxHIeBTZM1h6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Minibatch Gradient Descent\n",
        "\n",
        "- 전체 데이터를 균일하게 나눠서 학습하는 방법\n",
        "\n",
        "**장점**\n",
        "\n",
        "- 업데이터를 좀 더 빠르게 할 수 있다.\n",
        "\n",
        "**단점**\n",
        "\n",
        "- 전체 데이터를 쓰지 않아서 잘못된 방향으로 업데이트를 할 수 있다."
      ],
      "metadata": {
        "id": "4aJYGCI3PU7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch Dataset\n",
        "\n",
        "- torch.utils.data.Dataset 상속\n",
        "\n",
        "- \\_\\_len\\_\\_( )\n",
        "\n",
        "  - 이 데이터셋의 총 데이터 수\n",
        "\n",
        "- \\_\\_getitem\\_\\_()\n",
        "\n",
        "  - 어떠한 인덱스 idx를 받았을 때, 그에 상응하는 입출력 데이터 반환"
      ],
      "metadata": {
        "id": "SzDkY-eiP8pb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wp_hxco2Enje"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = [[73, 80, 75],\n",
        "                   [93, 88, 93],\n",
        "                   [89, 91, 90],\n",
        "                   [96, 98, 100],\n",
        "                   [73, 66, 70]]\n",
        "    self.y_data = [[152], [185], [180], [196], [142]]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    x = torch.FloatTensor(self.x_data[idx])\n",
        "    y = torch.FloatTensor(self.y_data[idx])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "dataset = CustomDataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch DataLoader\n",
        "\n",
        "- torch.utils.data.DataLoader 사용\n",
        "\n",
        "- batch_size = 2\n",
        "  - 각 minibatch의 크기\n",
        "  - 통상적으로 2의 제곱수로 설정(16, 32, 64,...)\n",
        "- shuffle = True\n",
        "  - Epoch 마다 데이터셋을 섞어서, 데이터가 학습되는 순서를 바꾼다."
      ],
      "metadata": {
        "id": "mfG4UdJoRg2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size = 2,\n",
        "    shuffle = True,\n",
        ")"
      ],
      "metadata": {
        "id": "aeKnjvSlRN_1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Code with Dataset and DataLoader\n",
        "\n",
        "- enumerate(dataloader)\n",
        "  - minibatch 인덱스와 데이터를 받음\n",
        "\n",
        "- len(dataloader)\n",
        "  - 한 epoch당 minibatch 개수"
      ],
      "metadata": {
        "id": "a1JetVt5SUpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultivariateLinearRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(3, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "metadata": {
        "id": "DuyauIZ0qfUx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultivariateLinearRegression()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 1e-5)\n",
        "nb_epochs = 20\n",
        "\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  for batch_idx, samples in enumerate(dataloader):\n",
        "    x_train, y_train = samples\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(epoch, nb_epochs,\n",
        "                                                           batch_idx + 1, len(dataloader),\n",
        "                                                           cost.item()))\n"
      ],
      "metadata": {
        "id": "eoFhfXm3SQ5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fca4f615-44d2-4882-eb84-717c8e510457"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/20 Batch 1/3 Cost: 35806.484375\n",
            "Epoch    0/20 Batch 2/3 Cost: 10742.810547\n",
            "Epoch    0/20 Batch 3/3 Cost: 4796.435547\n",
            "Epoch    1/20 Batch 1/3 Cost: 882.092163\n",
            "Epoch    1/20 Batch 2/3 Cost: 308.734467\n",
            "Epoch    1/20 Batch 3/3 Cost: 170.444794\n",
            "Epoch    2/20 Batch 1/3 Cost: 25.166571\n",
            "Epoch    2/20 Batch 2/3 Cost: 12.290089\n",
            "Epoch    2/20 Batch 3/3 Cost: 0.127282\n",
            "Epoch    3/20 Batch 1/3 Cost: 4.204286\n",
            "Epoch    3/20 Batch 2/3 Cost: 1.055140\n",
            "Epoch    3/20 Batch 3/3 Cost: 12.507796\n",
            "Epoch    4/20 Batch 1/3 Cost: 4.350006\n",
            "Epoch    4/20 Batch 2/3 Cost: 1.849484\n",
            "Epoch    4/20 Batch 3/3 Cost: 4.611588\n",
            "Epoch    5/20 Batch 1/3 Cost: 3.016401\n",
            "Epoch    5/20 Batch 2/3 Cost: 5.475317\n",
            "Epoch    5/20 Batch 3/3 Cost: 0.945437\n",
            "Epoch    6/20 Batch 1/3 Cost: 0.428300\n",
            "Epoch    6/20 Batch 2/3 Cost: 8.958086\n",
            "Epoch    6/20 Batch 3/3 Cost: 4.670693\n",
            "Epoch    7/20 Batch 1/3 Cost: 2.997018\n",
            "Epoch    7/20 Batch 2/3 Cost: 4.776351\n",
            "Epoch    7/20 Batch 3/3 Cost: 3.287756\n",
            "Epoch    8/20 Batch 1/3 Cost: 0.154215\n",
            "Epoch    8/20 Batch 2/3 Cost: 9.875568\n",
            "Epoch    8/20 Batch 3/3 Cost: 4.174033\n",
            "Epoch    9/20 Batch 1/3 Cost: 8.584578\n",
            "Epoch    9/20 Batch 2/3 Cost: 3.569812\n",
            "Epoch    9/20 Batch 3/3 Cost: 0.683796\n",
            "Epoch   10/20 Batch 1/3 Cost: 0.343018\n",
            "Epoch   10/20 Batch 2/3 Cost: 10.312834\n",
            "Epoch   10/20 Batch 3/3 Cost: 2.132154\n",
            "Epoch   11/20 Batch 1/3 Cost: 1.343385\n",
            "Epoch   11/20 Batch 2/3 Cost: 3.199141\n",
            "Epoch   11/20 Batch 3/3 Cost: 9.047946\n",
            "Epoch   12/20 Batch 1/3 Cost: 5.654048\n",
            "Epoch   12/20 Batch 2/3 Cost: 2.008312\n",
            "Epoch   12/20 Batch 3/3 Cost: 2.277365\n",
            "Epoch   13/20 Batch 1/3 Cost: 2.310427\n",
            "Epoch   13/20 Batch 2/3 Cost: 5.282541\n",
            "Epoch   13/20 Batch 3/3 Cost: 1.242931\n",
            "Epoch   14/20 Batch 1/3 Cost: 5.618792\n",
            "Epoch   14/20 Batch 2/3 Cost: 0.927811\n",
            "Epoch   14/20 Batch 3/3 Cost: 5.080850\n",
            "Epoch   15/20 Batch 1/3 Cost: 4.035289\n",
            "Epoch   15/20 Batch 2/3 Cost: 3.221895\n",
            "Epoch   15/20 Batch 3/3 Cost: 1.979557\n",
            "Epoch   16/20 Batch 1/3 Cost: 2.215588\n",
            "Epoch   16/20 Batch 2/3 Cost: 2.077277\n",
            "Epoch   16/20 Batch 3/3 Cost: 11.051850\n",
            "Epoch   17/20 Batch 1/3 Cost: 4.168535\n",
            "Epoch   17/20 Batch 2/3 Cost: 4.234344\n",
            "Epoch   17/20 Batch 3/3 Cost: 4.129821\n",
            "Epoch   18/20 Batch 1/3 Cost: 1.859411\n",
            "Epoch   18/20 Batch 2/3 Cost: 4.197704\n",
            "Epoch   18/20 Batch 3/3 Cost: 5.373304\n",
            "Epoch   19/20 Batch 1/3 Cost: 0.299851\n",
            "Epoch   19/20 Batch 2/3 Cost: 9.104072\n",
            "Epoch   19/20 Batch 3/3 Cost: 4.453648\n",
            "Epoch   20/20 Batch 1/3 Cost: 2.834744\n",
            "Epoch   20/20 Batch 2/3 Cost: 5.350664\n",
            "Epoch   20/20 Batch 3/3 Cost: 1.366041\n"
          ]
        }
      ]
    }
  ]
}